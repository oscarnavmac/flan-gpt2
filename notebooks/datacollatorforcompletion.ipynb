{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oscarn/flan-gpt2\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_utils import GPT2Model\n",
    "from data_utils import create_instruct_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "checkpoint = 'openai-community/gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = GPT2Model(checkpoint, device)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = gpt2_model.get_model()\n",
    "tokenizer = gpt2_model.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cade6304f9943b680234898df677811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load instruct dataset (4 tasks)\n",
    "datasets_names = [\"common_gen\"]\n",
    "dataset = create_instruct_dataset(datasets_names)\n",
    "\n",
    "# Tokenize examples\n",
    "tokenized_dataset = dataset.map(\n",
    "    gpt2_model.tokenize_function,\n",
    "    remove_columns=[\"prompt\", \"completion\"],\n",
    "    batched=True,\n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a sentence, and then tell me the concepts included in that sentence. ### Response: Sentence:\n",
      "The people are on mountain snowboarding down the slope.\n",
      "\n",
      "Concepts:\n",
      "['slope','snowboard','mountain']\n"
     ]
    }
   ],
   "source": [
    "example = tokenizer.decode(tokenized_dataset[\"input_ids\"][0])\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset, shuffle=False, batch_size=1, collate_fn=gpt2_model.get_collator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a sentence, and then tell me the concepts included in that sentence. ### Response: Sentence:\n",
      "The people are on mountain snowboarding down the slope.\n",
      "\n",
      "Concepts:\n",
      "['slope','snowboard','mountain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscarn/miniconda3/envs/transformers-311/lib/python3.11/site-packages/trl/trainer/utils.py:160: UserWarning: Could not find response key ` ### Response: ` in the following instance: Generate a sentence, and then tell me the concepts included in that sentence. ### Response: Sentence:\n",
      "The people are on mountain snowboarding down the slope.\n",
      "\n",
      "Concepts:\n",
      "['slope','snowboard','mountain'] This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "first_batch1 = next(iter(train_dataloader))\n",
    "example = tokenizer.decode(first_batch1[\"input_ids\"][0])\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataloader = DataLoader(\n",
    "    tokenized_dataset, shuffle=False, batch_size=1, collate_fn=DataCollatorForCompletionOnlyLM(tokenizer=tokenizer,response_template=[1593])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a sentence, and then tell me the concepts included in that sentence. ### Response: Sentence:\n",
      "The people are on mountain snowboarding down the slope.\n",
      "\n",
      "Concepts:\n",
      "['slope','snowboard','mountain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscarn/miniconda3/envs/transformers-311/lib/python3.11/site-packages/trl/trainer/utils.py:160: UserWarning: Could not find response key `[1593]` in the following instance: Generate a sentence, and then tell me the concepts included in that sentence. ### Response: Sentence:\n",
      "The people are on mountain snowboarding down the slope.\n",
      "\n",
      "Concepts:\n",
      "['slope','snowboard','mountain'] This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "first_batch2 = next(iter(sft_dataloader))\n",
    "example = tokenizer.decode(first_batch2[\"input_ids\"][0])\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_labels = first_batch2[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_labels = old_labels[old_labels != -100]\n",
    "example = tokenizer.decode(new_labels)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3103, 984, 82, 25, 37250, 67, 680, 3256, 705, 2655, 303, 3256, 705, 2118, 2899, 415, 20520, 198, 198, 16594, 257, 6827, 326, 3407, 477, 777, 2456, 13, 67, 680, 4983, 379, 262, 7072]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 680, 4983, 379, 262, 7072]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"targets\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44386, 18261, 25]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" ### Response:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 44386, 18261, 25, 685, 9310, 324]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello ### Response: [dsad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {\"pad_token\": \"<|pad|>\"}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oscarn/flan-gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscarn/miniconda3/envs/transformers-311/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from model_utils import GPT2Model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "checkpoint = 'OscarNav/gpt2-multitask-4_V2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2602b34aa6994aefb95e84e5bbad1fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/984 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323e128967df425cb28336611d4c65a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc7d077fabb49628955e7645414d603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32928920007249da8c64c333b9d4b31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/476 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96efc0915abb4162a8fc0256bb72a477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdebd3dabb284ae1b7bb097738ed9953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dadbf549f84a608b7e30825d11885a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e640a0de858549aa9d2f668e1f5a193c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2_model = GPT2Model(checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gpt2_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepo_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_temp_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcommit_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprivate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'5GB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcreate_pr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msafe_serialization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcommit_description\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mdeprecated_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Upload the model file to the 🤗 Model Hub.\n",
      "\n",
      "Parameters:\n",
      "    repo_id (`str`):\n",
      "        The name of the repository you want to push your model to. It should contain your organization name\n",
      "        when pushing to a given organization.\n",
      "    use_temp_dir (`bool`, *optional*):\n",
      "        Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      "        Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      "    commit_message (`str`, *optional*):\n",
      "        Message to commit while pushing. Will default to `\"Upload model\"`.\n",
      "    private (`bool`, *optional*):\n",
      "        Whether or not the repository created should be private.\n",
      "    token (`bool` or `str`, *optional*):\n",
      "        The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "        when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      "        is not specified.\n",
      "    max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      "        Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      "        will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      "        by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      "        Google Colab instances without any CPU OOM issues.\n",
      "    create_pr (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to create a PR with the uploaded files or directly commit.\n",
      "    safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      "    revision (`str`, *optional*):\n",
      "        Branch to push the uploaded files to.\n",
      "    commit_description (`str`, *optional*):\n",
      "        The description of the commit that will be created\n",
      "    tags (`List[str]`, *optional*):\n",
      "        List of tags to push on the Hub.\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      "from transformers import AutoModel\n",
      "\n",
      "model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n",
      "\n",
      "# Push the model to your namespace with the name \"my-finetuned-bert\".\n",
      "model.push_to_hub(\"my-finetuned-bert\")\n",
      "\n",
      "# Push the model to an organization with the name \"my-finetuned-bert\".\n",
      "model.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/transformers-311/lib/python3.11/site-packages/transformers/utils/hub.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.push_to_hub?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 22:28:15,495 - datasets - INFO - PyTorch version 2.5.1 available.\n"
     ]
    }
   ],
   "source": [
    "from models.model_utils import GPT2Model\n",
    "import torch\n",
    "from data.data_utils import create_instruct_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(10, [\"eng_spa\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Translate the following.\\n\\nEnglish: Tom was the one who cooked dinner.\\n\\nSpanish:',\n",
       " 'completion': 'Tom fue el que cocinó la cena.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wrapped_model = GPT2Model(\"openai-community/gpt2-medium\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b66c5f7a0a48f2a929fbd613fd0514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'targets'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_func = wrapped_model.tokenize_function\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenizer_func,\n",
    "    remove_columns=[\"prompt\", \"completion\"],\n",
    "    batched=True,\n",
    "    batch_size=5\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8291,\n",
       "  17660,\n",
       "  262,\n",
       "  1708,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  15823,\n",
       "  25,\n",
       "  4186,\n",
       "  373,\n",
       "  262,\n",
       "  530,\n",
       "  508,\n",
       "  15847,\n",
       "  8073,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  43584,\n",
       "  25,\n",
       "  13787,\n",
       "  37911,\n",
       "  1288,\n",
       "  8358,\n",
       "  8954,\n",
       "  259,\n",
       "  10205,\n",
       "  8591,\n",
       "  269,\n",
       "  8107,\n",
       "  13,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'targets': [13787, 37911, 1288, 8358, 8954, 259, 10205, 8591, 269, 8107, 13]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    #text = example[\"prompt\"] + \"\\n\" + example[\"completion\"] -> OLD WAY\n",
    "    text = [p + c + tokenizer.eos_token for p, c in zip(example[\"prompt\"], example[\"completion\"])]\n",
    "    # join list of strings\n",
    "    text = \"\".join(text)\n",
    "    input_encodings = tokenizer(text, truncation=True) #Maybe dont use truncation\n",
    "    target_encodings = tokenizer(example[\"completion\"], truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"targets\": target_encodings[\"input_ids\"]} #because its necesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15751,\n",
       "  50256,\n",
       "  305,\n",
       "  50256,\n",
       "  321,\n",
       "  50256,\n",
       "  77,\n",
       "  220,\n",
       "  50256,\n",
       "  28202,\n",
       "  50256,\n",
       "  2290,\n",
       "  50256,\n",
       "  3609,\n",
       "  50256,\n",
       "  83,\n",
       "  220,\n",
       "  50256,\n",
       "  1453,\n",
       "  50256,\n",
       "  300,\n",
       "  50256,\n",
       "  83,\n",
       "  220,\n",
       "  50256,\n",
       "  71,\n",
       "  80,\n",
       "  50256,\n",
       "  12496,\n",
       "  50256,\n",
       "  304,\n",
       "  50256,\n",
       "  69,\n",
       "  220,\n",
       "  50256,\n",
       "  420,\n",
       "  50256,\n",
       "  5439,\n",
       "  50256,\n",
       "  44601,\n",
       "  50256,\n",
       "  23013,\n",
       "  50256,\n",
       "  675,\n",
       "  50256,\n",
       "  72,\n",
       "  10205,\n",
       "  50256,\n",
       "  77,\n",
       "  220,\n",
       "  50256,\n",
       "  4743,\n",
       "  50256,\n",
       "  13,\n",
       "  64,\n",
       "  50256,\n",
       "  198,\n",
       "  220,\n",
       "  50256,\n",
       "  198,\n",
       "  66,\n",
       "  50256,\n",
       "  36,\n",
       "  68,\n",
       "  50256,\n",
       "  20471,\n",
       "  50256,\n",
       "  4908,\n",
       "  50256,\n",
       "  75,\n",
       "  13,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'targets': [13787, 37911, 1288, 8358, 8954, 259, 10205, 8591, 269, 8107, 13]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[15751, 50256],\n",
       "  [305, 50256],\n",
       "  [321, 50256],\n",
       "  [77, 220, 50256],\n",
       "  [28202, 50256],\n",
       "  [2290, 50256],\n",
       "  [3609, 50256],\n",
       "  [83, 220, 50256],\n",
       "  [1453, 50256],\n",
       "  [300, 50256],\n",
       "  [83, 220, 50256],\n",
       "  [71, 80, 50256],\n",
       "  [12496, 50256],\n",
       "  [304, 50256],\n",
       "  [69, 220, 50256],\n",
       "  [420, 50256],\n",
       "  [5439, 50256],\n",
       "  [44601, 50256],\n",
       "  [23013, 50256],\n",
       "  [675, 50256],\n",
       "  [72, 10205, 50256],\n",
       "  [77, 220, 50256],\n",
       "  [4743, 50256],\n",
       "  [13, 64, 50256],\n",
       "  [198, 220, 50256],\n",
       "  [198, 66, 50256],\n",
       "  [36, 68, 50256],\n",
       "  [20471, 50256],\n",
       "  [4908, 50256],\n",
       "  [75, 13, 50256]],\n",
       " 'attention_mask': [[1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1]],\n",
       " 'targets': [13787, 37911, 1288, 8358, 8954, 259, 10205, 8591, 269, 8107, 13]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_func(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the following.\\n\\nEnglish: Tom was the one who cooked dinner.\\n\\nSpanish:Tom fue el que cocinó la cena.<|endoftext|>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TT<|endoftext|>ro<|endoftext|>am<|endoftext|>n <|endoftext|>sf<|endoftext|>lu<|endoftext|>ae<|endoftext|>t <|endoftext|>ee<|endoftext|> l<|endoftext|>t <|endoftext|>hq<|endoftext|>eu<|endoftext|> e<|endoftext|>f <|endoftext|>oc<|endoftext|>lo<|endoftext|>lc<|endoftext|>oi<|endoftext|>wn<|endoftext|>ió<|endoftext|>n <|endoftext|>gl<|endoftext|>.a<|endoftext|>\\n <|endoftext|>\\nc<|endoftext|>Ee<|endoftext|>nn<|endoftext|>ga<|endoftext|>l.<|endoftext|>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenize_function(dataset[0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_utils import T5Model\n",
    "wrapped_model2 = T5Model(\"google/flan-t5-base\", device)\n",
    "tokenizer_func2 = wrapped_model2.tokenize_function\n",
    "tokenizer2 = wrapped_model2.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [30355,\n",
       "  15,\n",
       "  8,\n",
       "  826,\n",
       "  5,\n",
       "  1566,\n",
       "  10,\n",
       "  3059,\n",
       "  47,\n",
       "  8,\n",
       "  80,\n",
       "  113,\n",
       "  8311,\n",
       "  2634,\n",
       "  5,\n",
       "  5093,\n",
       "  10,\n",
       "  1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [3059,\n",
       "  7683,\n",
       "  15,\n",
       "  3,\n",
       "  15,\n",
       "  40,\n",
       "  238,\n",
       "  576,\n",
       "  75,\n",
       "  77,\n",
       "  4922,\n",
       "  50,\n",
       "  197,\n",
       "  29,\n",
       "  9,\n",
       "  5,\n",
       "  1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_func2(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the following. English: Tom was the one who cooked dinner. Spanish:</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(tokenizer_func2(dataset[0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6718089a05f24efe93839c1199c0496d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset2 = dataset.map(\n",
    "    tokenizer_func2,\n",
    "    remove_columns=[\"prompt\", \"completion\"],\n",
    "    batched=True,\n",
    "    batch_size=500\n",
    ")\n",
    "tokenized_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [30355,\n",
       "  15,\n",
       "  8,\n",
       "  826,\n",
       "  5,\n",
       "  1566,\n",
       "  10,\n",
       "  3059,\n",
       "  47,\n",
       "  8,\n",
       "  80,\n",
       "  113,\n",
       "  8311,\n",
       "  2634,\n",
       "  5,\n",
       "  5093,\n",
       "  10,\n",
       "  1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [3059,\n",
       "  7683,\n",
       "  15,\n",
       "  3,\n",
       "  15,\n",
       "  40,\n",
       "  238,\n",
       "  576,\n",
       "  75,\n",
       "  77,\n",
       "  4922,\n",
       "  50,\n",
       "  197,\n",
       "  29,\n",
       "  9,\n",
       "  5,\n",
       "  1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
