{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oscarn/flan-gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscarn/miniconda3/envs/transformers-311/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-multitask-4_V2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from eval_utils import Evaluation\n",
    "from data_utils import format_example, format_options\n",
    "from templates import PATTERNS\n",
    "import csv\n",
    "\n",
    "eval = Evaluation(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashline = '-'.join('' for x in range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluacion Cualitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_list, return_full_text=True):\n",
    "    outputs = []\n",
    "    for input in input_list:\n",
    "        #input += \" ### Response: \"\n",
    "        inputs = tokenizer(input, return_tensors='pt').to(device)\n",
    "        input_length = len(tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "        output = tokenizer.decode(\n",
    "            model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                pad_token_id=60000,\n",
    "                eos_token_id=60001,\n",
    "                max_new_tokens=40,\n",
    "                do_sample=True\n",
    "            )[0],\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if return_full_text: \n",
    "            outputs.append(output) \n",
    "        else: \n",
    "            outputs.append(output[input_length:].strip())\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response... :   0%|          | 0/1 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating response... : 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT \n",
      "Ernest Jones is a British jeweller and watchmaker. Established in 1949, its first store was opened in Oxford Street, London. Ernest Jones specialises in diamonds and watches, stocking brands such as Gucci and Emporio Armani. Ernest Jones is part of the Signet Jewelers group.\n",
      "\n",
      "Based on the paragraph above can we conclude that \"The first Ernest Jones store was opened on the continent of Europe.\"?\n",
      "\n",
      "OPTIONS:\n",
      "- entailment\n",
      "- neutral\n",
      "- contradiction\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Ground Truth: \n",
      "\n",
      "entailment\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION: \n",
      "\n",
      "neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"facebook/anli\", split=\"test_r1\")\n",
    "int2str = dataset.features['label'].int2str\n",
    "dataset = dataset.map(lambda example: {\"answer\": int2str(example[\"label\"])})\n",
    "options = [[\"entailment\", \"neutral\", \"contradiction\"]] * len(dataset)\n",
    "dataset = dataset.add_column(\"options\", options).map(format_options)\n",
    "example = dataset[0]\n",
    "prompt, ground_truth = format_example(example, PATTERNS[\"anli\"], 0).values()\n",
    "prediction = eval.generate([prompt], return_full_text=False)[0]\n",
    "print(dashline)\n",
    "print(f'INPUT PROMPT \\n{prompt}')\n",
    "print(dashline)\n",
    "print(f\"Ground Truth: \\n\\n{ground_truth}\")\n",
    "print(dashline)\n",
    "print(f'MODEL GENERATION: \\n\\n{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response... : 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT \n",
      "All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "\n",
      "Can we conclude that does ethanol take more energy make that produces?\n",
      "\n",
      "OPTIONS:\n",
      "- True\n",
      "- False\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Ground Truth: \n",
      "\n",
      "False\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION: \n",
      "\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('google/boolq', split='validation')\n",
    "options = [[\"True\", \"False\"]] * len(dataset)\n",
    "dataset = dataset.add_column(\"options\", options).map(format_options)\n",
    "example = dataset[0]\n",
    "prompt, ground_truth = format_example(example, PATTERNS[\"bool_q\"], 0).values()\n",
    "prediction = eval.generate([prompt], return_full_text=False)[0]\n",
    "print(dashline)\n",
    "print(f'INPUT PROMPT \\n{prompt}')\n",
    "print(dashline)\n",
    "print(f\"Ground Truth: \\n\\n{ground_truth}\")\n",
    "print(dashline)\n",
    "print(f'MODEL GENERATION: \\n\\n{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response... : 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT \n",
      "Concepts: ['field', 'look', 'stand']\n",
      "\n",
      "Write a sentence that includes all these words.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Ground Truth: \n",
      "\n",
      "The player stood in the field looking at the batter.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION: \n",
      "\n",
      "boy looking at the farmers farm in the countryside\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('allenai/common_gen', split='validation')\n",
    "example = dataset[0]\n",
    "prompt, ground_truth = format_example(example, PATTERNS[\"common_gen\"], 0).values()\n",
    "prediction = eval.generate([prompt], return_full_text=False)[0]\n",
    "print(dashline)\n",
    "print(f'INPUT PROMPT \\n{prompt}')\n",
    "print(dashline)\n",
    "print(f\"Ground Truth: \\n\\n{ground_truth}\")\n",
    "print(dashline)\n",
    "print(f'MODEL GENERATION: \\n\\n{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response... : 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT \n",
      "Summarize:\n",
      "\n",
      "Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\n",
      "Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\n",
      "The Welsh Government said more people than ever were getting help to address housing problems.\n",
      "Changes to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.\n",
      "Prison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.\n",
      "However, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.\n",
      "Andrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".\n",
      "\"There's a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.\n",
      "\"It could take six months to a year, without a lot of help they could be on the streets for six months.\n",
      "\"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\"\n",
      "Mr Stevens believes building more one-bedroom flats could help ease the problem.\n",
      "\"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said.\n",
      "Official figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales.\n",
      "Marc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released.\n",
      "He said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\"\n",
      "\"You're put out among the same sort of people doing the same sort of thing, and it's difficult, it's difficult to get away from it. It's like every man for himself, there's nothing.\"\n",
      "Marc has now found stable accommodation with homeless charity Emmaus and said it had been life changing.\n",
      "\"You feel safe, you got hot food, you've got company of people in similar situations to yourself but all dealing with different issues. It's a constructive, helpful atmosphere,\" he said.\n",
      "Tom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available.\n",
      "\"We do still see [people] homeless on the streets, so clearly they haven't got accommodation and haven't got provision,\" he said.\n",
      "\"I think the key is connecting people with the services they need. I don't delude myself that Emmaus can offer a one size fits all for everyone, we can't.\n",
      "\"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\"\n",
      "A Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence.\n",
      "It added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Ground Truth: \n",
      "\n",
      "There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION: \n",
      "\n",
      "\"We will continue to work closely with all bodies to understand the benefits of building up a national pathway that is a more resilient solution, for communities facing housing affordability and homelessness as well as to develop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('EdinburghNLP/xsum', split='test')\n",
    "example = dataset[0]\n",
    "prompt, ground_truth = format_example(example, PATTERNS[\"xsum\"], 0).values()\n",
    "prediction = eval.generate([prompt], return_full_text=False)[0]\n",
    "print(dashline)\n",
    "print(f'INPUT PROMPT \\n{prompt}')\n",
    "print(dashline)\n",
    "print(f\"Ground Truth: \\n\\n{ground_truth}\")\n",
    "print(dashline)\n",
    "print(f'MODEL GENERATION: \\n\\n{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacion Cuantitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response... : 100%|██████████| 500/500 [00:49<00:00, 10.07it/s]\n",
      "Generating response... : 100%|██████████| 500/500 [00:31<00:00, 15.91it/s]\n",
      "Generating response... : 100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "Generating response... : 100%|██████████| 368/368 [06:42<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "res_anli = eval.anli(500, return_full_text=False)\n",
    "res_boolq = eval.bool_q(500, return_full_text=False)\n",
    "res_commongen = eval.common_gen(500, return_full_text=False)\n",
    "res_xsum = eval.xsum(500, return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy on ANLI is 0.36\n",
      "Total accuracy on BoolQ is 0.552\n",
      "Rouge-1 score on Common Gen is 0.24627059277106764\n",
      "Rouge-LSum score on XSum is 0.1213437935936966\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total accuracy on ANLI is {res_anli}\")\n",
    "print(f\"Total accuracy on BoolQ is {res_boolq}\")\n",
    "print(f\"Rouge-1 score on Common Gen is {res_commongen}\")\n",
    "print(f\"Rouge-LSum score on XSum is {res_xsum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [model_name, res_anli, res_boolq, float(res_commongen), float(res_xsum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt2-multitask-4_V2', 0.36, 0.552, 0.24627059277106764, 0.1213437935936966]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to results.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'results.csv'\n",
    "# Appending to CSV file\n",
    "with open(filename, 'a', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(results)\n",
    "print(f\"Data has been written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OscarNav/flan-gpt2-distill-test', 0.002, 0.0, 0.0005714285714285714, 0.00045606567345697776]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615f06ccab1449deac29c982b709208d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ca7d914b9248938eb5dbb3a85f9909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/OscarNav/flan-gpt2-distill-test/commit/dd74c48d5441fde34c9970c5a2339b98e4feb55f', commit_message='Upload tokenizer', commit_description='', oid='dd74c48d5441fde34c9970c5a2339b98e4feb55f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/OscarNav/flan-gpt2-distill-test', endpoint='https://huggingface.co', repo_type='model', repo_id='OscarNav/flan-gpt2-distill-test'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"OscarNav/flan-gpt2-distill-test\")\n",
    "tokenizer.push_to_hub(\"OscarNav/flan-gpt2-distill-test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
