{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oscarn/flan-gpt2\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2-medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from models.model_utils import GPT2Model\n",
    "\n",
    "wrapped_model = GPT2Model(\"output/gpt2-lora\", device, peft=True)\n",
    "peft_model = wrapped_model.get_model(peft=True)\n",
    "peft_tokenizer = wrapped_model.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.eval_utils import Evaluation\n",
    "from data.data_utils import create_instruct_dataset\n",
    "\n",
    "\n",
    "eval = Evaluation(peft_model, peft_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "CAUSAL_LM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "## INPUT PROMPT\n",
    "{prompt}\n",
    "## GROUND_TRUTH\n",
    "{completion}\n",
    "## MODEL OUTPUT\n",
    "{prediction}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b7bfea51f04c748215926ab3bcb3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1080be13388e4b35a9ec1f485e0295aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65b4eb558e7438189b047dc1b69ef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Read the following paragraph and determine if the hypothesis is true:                                              \n",
       "\n",
       "The 1947 Washington State Cougars football team was an American football team that represented Washington State    \n",
       "College in the Pacific Coast Conference (PCC) during the 1947 college football season. Phil Sarboe, in his third of\n",
       "five seasons as head coach at Washington State, led the team to a 2–5 mark in the PCC and 3–7 overall.             \n",
       "\n",
       "Hypothesis: Sarboe led a mark of more than one                                                                     \n",
       "\n",
       "OPTIONS:                                                                                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>entailment                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>neutral                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>contradiction                                                                                                   \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "entailment                                                                                                         \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "entailment                                                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Read the following paragraph and determine if the hypothesis is true:                                              \n",
       "\n",
       "The 1947 Washington State Cougars football team was an American football team that represented Washington State    \n",
       "College in the Pacific Coast Conference (PCC) during the 1947 college football season. Phil Sarboe, in his third of\n",
       "five seasons as head coach at Washington State, led the team to a 2–5 mark in the PCC and 3–7 overall.             \n",
       "\n",
       "Hypothesis: Sarboe led a mark of more than one                                                                     \n",
       "\n",
       "OPTIONS:                                                                                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0mentailment                                                                                                      \n",
       "\u001b[1;33m • \u001b[0mneutral                                                                                                         \n",
       "\u001b[1;33m • \u001b[0mcontradiction                                                                                                   \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "entailment                                                                                                         \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "entailment                                                                                                         \n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"anli\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Generate a sentence that includes all the following words: ['station', 'train', 'pull']                            \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "the train pulling into station                                                                                     \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "pull a train into a station                                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Generate a sentence that includes all the following words: ['station', 'train', 'pull']                            \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "the train pulling into station                                                                                     \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "pull a train into a station                                                                                        \n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"common_gen\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rajpurkar/squad couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /home/oscarn/.cache/huggingface/datasets/rajpurkar___squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f (last modified on Fri Nov 22 00:36:36 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fa059c94d4414a9afa40753ec1fc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e5d51b311f4ca9aea11a0e13b79098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ad079d63c14d66b0835ae3d5e6b98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0290cd549c7a4fa2ac675a5740fa83b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Article: In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138        \n",
       "graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. Around 21–24% of students\n",
       "are children of alumni, and although 37% of students come from the Midwestern United States, the student body      \n",
       "represents all 50 states and 100 countries. As of March 2007[update] The Princeton Review ranked the school as the \n",
       "fifth highest 'dream school' for parents to send their children. As of March 2015[update] The Princeton Review     \n",
       "ranked Notre Dame as the ninth highest. The school has been previously criticized for its lack of diversity, and   \n",
       "The Princeton Review ranks the university highly among schools at which \"Alternative Lifestyles [are] Not an       \n",
       "Alternative.\" It has also been commended by some diversity oriented publications; Hispanic Magazine in 2004 ranked \n",
       "the university ninth on its list of the top–25 colleges for Latinos, and The Journal of Blacks in Higher Education \n",
       "recognized the university in 2006 for raising enrollment of African-American students. With 6,000 participants, the\n",
       "university's intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, \n",
       "while in 2007 The Princeton Review named it as the top school where \"Everyone Plays Intramural Sports.\" The annual \n",
       "Bookstore Basketball tournament is the largest outdoor five-on-five tournament in the world with over 700 teams    \n",
       "participating each year, while the Notre Dame Men's Boxing Club hosts the annual Bengal Bouts tournament that      \n",
       "raises money for the Holy Cross Missions in Bangladesh.                                                            \n",
       "\n",
       "Now answer this question: How many teams participate in the Notre Dame Bookstore Basketball tournament?            \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "over 700                                                                                                           \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "12                                                                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Article: In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138        \n",
       "graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. Around 21–24% of students\n",
       "are children of alumni, and although 37% of students come from the Midwestern United States, the student body      \n",
       "represents all 50 states and 100 countries. As of March 2007[update] The Princeton Review ranked the school as the \n",
       "fifth highest 'dream school' for parents to send their children. As of March 2015[update] The Princeton Review     \n",
       "ranked Notre Dame as the ninth highest. The school has been previously criticized for its lack of diversity, and   \n",
       "The Princeton Review ranks the university highly among schools at which \"Alternative Lifestyles [are] Not an       \n",
       "Alternative.\" It has also been commended by some diversity oriented publications; Hispanic Magazine in 2004 ranked \n",
       "the university ninth on its list of the top–25 colleges for Latinos, and The Journal of Blacks in Higher Education \n",
       "recognized the university in 2006 for raising enrollment of African-American students. With 6,000 participants, the\n",
       "university's intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, \n",
       "while in 2007 The Princeton Review named it as the top school where \"Everyone Plays Intramural Sports.\" The annual \n",
       "Bookstore Basketball tournament is the largest outdoor five-on-five tournament in the world with over 700 teams    \n",
       "participating each year, while the Notre Dame Men's Boxing Club hosts the annual Bengal Bouts tournament that      \n",
       "raises money for the Holy Cross Missions in Bangladesh.                                                            \n",
       "\n",
       "Now answer this question: How many teams participate in the Notre Dame Bookstore Basketball tournament?            \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "over 700                                                                                                           \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "12                                                                                                                 \n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"squad\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosmos QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/oscarn/.cache/huggingface/modules/datasets_modules/datasets/allenai--cosmos_qa/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157 (last modified on Fri Nov 22 00:37:03 2024) since it couldn't be found locally at allenai/cosmos_qa, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa553e822b8d47378834cefb4f1ab1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c371df4dbca44dd1a18021113be117fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55062ee5a758450abb3a0c1531d6a0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "WHAT could be in the package ? Something PORNOGRAPHIC , that 's for sure ! I rushed into the house and went to the \n",
       "laundry room , closed the door and pulled the package out of the magazine . I flipped it over , and reread the     \n",
       "words on the side . Contains PHOTOGRAPHIC material . Damn .                                                        \n",
       "\n",
       "Generate a question about the above context.                                                                       \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "Why is this person disappointed that they misread the label on the package ?                                       \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "What?                                                                                                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "WHAT could be in the package ? Something PORNOGRAPHIC , that 's for sure ! I rushed into the house and went to the \n",
       "laundry room , closed the door and pulled the package out of the magazine . I flipped it over , and reread the     \n",
       "words on the side . Contains PHOTOGRAPHIC material . Damn .                                                        \n",
       "\n",
       "Generate a question about the above context.                                                                       \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "Why is this person disappointed that they misread the label on the package ?                                       \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "What?                                                                                                              \n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"cosmos_qa\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4db14543a748568d43086e6b33be63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0d17c752934cacbde1da7505ad9ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae6711c6a914abd9d76593e39af00dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1113373f7b8b4b6a847a0261de379287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:05<00:00,  5.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Answer the questions at the end based on the text.                                                                 \n",
       "\n",
       "My dad runs the Blue Street Zoo. Everyone calls him the Zoo King. That means Mom is the Zoo Queen. And that means  \n",
       "that I'm the Zoo Prince! Being a prince is very special.                                                           \n",
       "\n",
       "I spend every morning walking around to see the zoo. It's better than any animal book. I say hello to the lions. I \n",
       "say woof at all of the wolves. I make faces to the penguins. Once I even gave a morning kiss to a bear! My favorite\n",
       "animal is the piggy. I named him Samson. He likes to eat mustard, so I toss some mustard jars into his cage every  \n",
       "morning. I don't know why that piggy likes mustard so much.                                                        \n",
       "\n",
       "Sometimes I walk around with the Zoo King and Zoo Queen. Then we say hello to the animals together! I really like  \n",
       "those days. Everybody who works at the Zoo says hello to us when we walk by. At lunchtime, we all go to the Zoo    \n",
       "restaurant and eat pork chops. I hope Samson doesn't get mad about that!                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  1 </span>Who runs the zoo?                                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  2 </span>And what is he called?                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  3 </span>Who has been kissed?                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  4 </span>Who gets woofed at?                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  5 </span>Who is the favorite animal?                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  6 </span>Is it male or female?                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  7 </span>What is his name?                                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  8 </span>What does he like to eat?                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  9 </span>Where did they eat lunch?                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 10 </span>What did they eat?                                                                                             \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  1 </span>Dad                                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  2 </span>The Zoo King.                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  3 </span>a bear                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  4 </span>the wolves.                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  5 </span>the piggy.                                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  6 </span>Male                                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  7 </span>Samson.                                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  8 </span>mustard,                                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">  9 </span>the Zoo restaurant                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 10 </span>pork chops.                                                                                                    \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 11 </span>What animal was his favorite?                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 12 </span>What is his nickname?                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 13 </span>Did someone say hello on their way to work?                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 14 </span>The Purple Street Zoo                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 15 </span>My dad                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 16 </span>The zoo king                                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 17 </span>Zoo Queen                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 18 </span>Samson                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 19 </span>the lions                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 20 </span>piggy                                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 21 </span>bears                                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 22 </span>No                                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 23 </span>No                                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 24 </span>No                                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 25 </span>No                                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 26 </span>Yes.                                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 27 </span>No                                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 28 </span>Yes                                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 29 </span>yes                                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 30 </span>Yes                                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Answer the questions at the end based on the text.                                                                 \n",
       "\n",
       "My dad runs the Blue Street Zoo. Everyone calls him the Zoo King. That means Mom is the Zoo Queen. And that means  \n",
       "that I'm the Zoo Prince! Being a prince is very special.                                                           \n",
       "\n",
       "I spend every morning walking around to see the zoo. It's better than any animal book. I say hello to the lions. I \n",
       "say woof at all of the wolves. I make faces to the penguins. Once I even gave a morning kiss to a bear! My favorite\n",
       "animal is the piggy. I named him Samson. He likes to eat mustard, so I toss some mustard jars into his cage every  \n",
       "morning. I don't know why that piggy likes mustard so much.                                                        \n",
       "\n",
       "Sometimes I walk around with the Zoo King and Zoo Queen. Then we say hello to the animals together! I really like  \n",
       "those days. Everybody who works at the Zoo says hello to us when we walk by. At lunchtime, we all go to the Zoo    \n",
       "restaurant and eat pork chops. I hope Samson doesn't get mad about that!                                           \n",
       "\n",
       "\u001b[1;33m  1 \u001b[0mWho runs the zoo?                                                                                              \n",
       "\u001b[1;33m  2 \u001b[0mAnd what is he called?                                                                                         \n",
       "\u001b[1;33m  3 \u001b[0mWho has been kissed?                                                                                           \n",
       "\u001b[1;33m  4 \u001b[0mWho gets woofed at?                                                                                            \n",
       "\u001b[1;33m  5 \u001b[0mWho is the favorite animal?                                                                                    \n",
       "\u001b[1;33m  6 \u001b[0mIs it male or female?                                                                                          \n",
       "\u001b[1;33m  7 \u001b[0mWhat is his name?                                                                                              \n",
       "\u001b[1;33m  8 \u001b[0mWhat does he like to eat?                                                                                      \n",
       "\u001b[1;33m  9 \u001b[0mWhere did they eat lunch?                                                                                      \n",
       "\u001b[1;33m 10 \u001b[0mWhat did they eat?                                                                                             \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "\u001b[1;33m  1 \u001b[0mDad                                                                                                            \n",
       "\u001b[1;33m  2 \u001b[0mThe Zoo King.                                                                                                  \n",
       "\u001b[1;33m  3 \u001b[0ma bear                                                                                                         \n",
       "\u001b[1;33m  4 \u001b[0mthe wolves.                                                                                                    \n",
       "\u001b[1;33m  5 \u001b[0mthe piggy.                                                                                                     \n",
       "\u001b[1;33m  6 \u001b[0mMale                                                                                                           \n",
       "\u001b[1;33m  7 \u001b[0mSamson.                                                                                                        \n",
       "\u001b[1;33m  8 \u001b[0mmustard,                                                                                                       \n",
       "\u001b[1;33m  9 \u001b[0mthe Zoo restaurant                                                                                             \n",
       "\u001b[1;33m 10 \u001b[0mpork chops.                                                                                                    \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "\u001b[1;33m 11 \u001b[0mWhat animal was his favorite?                                                                                  \n",
       "\u001b[1;33m 12 \u001b[0mWhat is his nickname?                                                                                          \n",
       "\u001b[1;33m 13 \u001b[0mDid someone say hello on their way to work?                                                                    \n",
       "\u001b[1;33m 14 \u001b[0mThe Purple Street Zoo                                                                                          \n",
       "\u001b[1;33m 15 \u001b[0mMy dad                                                                                                         \n",
       "\u001b[1;33m 16 \u001b[0mThe zoo king                                                                                                   \n",
       "\u001b[1;33m 17 \u001b[0mZoo Queen                                                                                                      \n",
       "\u001b[1;33m 18 \u001b[0mSamson                                                                                                         \n",
       "\u001b[1;33m 19 \u001b[0mthe lions                                                                                                      \n",
       "\u001b[1;33m 20 \u001b[0mpiggy                                                                                                          \n",
       "\u001b[1;33m 21 \u001b[0mbears                                                                                                          \n",
       "\u001b[1;33m 22 \u001b[0mNo                                                                                                             \n",
       "\u001b[1;33m 23 \u001b[0mNo                                                                                                             \n",
       "\u001b[1;33m 24 \u001b[0mNo                                                                                                             \n",
       "\u001b[1;33m 25 \u001b[0mNo                                                                                                             \n",
       "\u001b[1;33m 26 \u001b[0mYes.                                                                                                           \n",
       "\u001b[1;33m 27 \u001b[0mNo                                                                                                             \n",
       "\u001b[1;33m 28 \u001b[0mYes                                                                                                            \n",
       "\u001b[1;33m 29 \u001b[0myes                                                                                                            \n",
       "\u001b[1;33m 30 \u001b[0mYes                                                                                                            \n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"coqa\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], max_tokens=150, return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since EdinburghNLP/xsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/oscarn/.cache/huggingface/datasets/EdinburghNLP___xsum/default/1.2.0/40db7604fedb616a9d2b0673d11838fa5be8451c (last modified on Sat Sep  6 17:08:46 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73044644d22e47ce86759c5616a23835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Summarize:                                                                                                         \n",
       "\n",
       "The pair will be taking over the job from Sarah-Jane Crawford, who only hosted the show for one series. This year's\n",
       "X Factor will see big changes, with judges Louis Walsh, Mel B and presenter Dermot O'Leary leaving. Simon Cowell   \n",
       "and Cheryl Fernandez-Versini are the only faces returning. Olly Murs and Caroline Flack have already been named as \n",
       "The X Factor's new presenters. The judging panel will be completed by newcomers Nick Grimshaw and Rita Ora, who was\n",
       "poached from BBC One's The Voice. Rochelle's new co-host Melvin Odoom is best known for presenting Kiss FM's       \n",
       "breakfast show in London since 2007. The Xtra Factor is a spin-off show to the main weekend programme, and follows \n",
       "the backstage action, as well as chatting with judges and contestants. Humes said: \"I have watched The Xtra Factor \n",
       "for years so I am beyond excited about joining such a brilliant team and getting to work with my old friend Melvin \n",
       "makes it even better. \"I also can't wait to meet the contestants and be a part of their X Factor experience.\"      \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "Saturdays singer Rochelle Humes is the new host of The X Factor spin-off show The Xtra Factor, where she has been  \n",
       "partnered with radio DJ Melvin Odoom.                                                                              \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "The show will be broadcast on BBC Two at 4:30pm on Sunday and Tuesday on BBC One.The X-Factor airs on BBC Two on   \n",
       "Sunday and Tuesday on BBC One                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Summarize:                                                                                                         \n",
       "\n",
       "The pair will be taking over the job from Sarah-Jane Crawford, who only hosted the show for one series. This year's\n",
       "X Factor will see big changes, with judges Louis Walsh, Mel B and presenter Dermot O'Leary leaving. Simon Cowell   \n",
       "and Cheryl Fernandez-Versini are the only faces returning. Olly Murs and Caroline Flack have already been named as \n",
       "The X Factor's new presenters. The judging panel will be completed by newcomers Nick Grimshaw and Rita Ora, who was\n",
       "poached from BBC One's The Voice. Rochelle's new co-host Melvin Odoom is best known for presenting Kiss FM's       \n",
       "breakfast show in London since 2007. The Xtra Factor is a spin-off show to the main weekend programme, and follows \n",
       "the backstage action, as well as chatting with judges and contestants. Humes said: \"I have watched The Xtra Factor \n",
       "for years so I am beyond excited about joining such a brilliant team and getting to work with my old friend Melvin \n",
       "makes it even better. \"I also can't wait to meet the contestants and be a part of their X Factor experience.\"      \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "Saturdays singer Rochelle Humes is the new host of The X Factor spin-off show The Xtra Factor, where she has been  \n",
       "partnered with radio DJ Melvin Odoom.                                                                              \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "The show will be broadcast on BBC Two at 4:30pm on Sunday and Tuesday on BBC One.The X-Factor airs on BBC Two on   \n",
       "Sunday and Tuesday on BBC One                                                                                      \n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"xsum\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24771533ba74c4cbe71c6b746d3c5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bfe0d83d7a4123a4dcbd3bd88306b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335472a973124173b0a352b0e0cc6b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782791335b7349f6a234c4efbf6e8d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Is it true that does any other country have daylight savings time based on the following text?                     \n",
       "\n",
       "Most areas in North America and Europe, and some areas in the Middle East, observe daylight saving time (DST),     \n",
       "while most areas of Africa and Asia do not. In South America, most countries in the north of the continent near the\n",
       "equator do not observe DST, while Paraguay and southern parts of Brazil do. The practice of observing daylight     \n",
       "saving time in Oceania is also mixed, with New Zealand and parts of southeastern Australia observing DST, while    \n",
       "most other areas do not.                                                                                           \n",
       "\n",
       "OPTIONS:                                                                                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>True                                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>False                                                                                                           \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "True                                                                                                               \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "True                                                                                                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Is it true that does any other country have daylight savings time based on the following text?                     \n",
       "\n",
       "Most areas in North America and Europe, and some areas in the Middle East, observe daylight saving time (DST),     \n",
       "while most areas of Africa and Asia do not. In South America, most countries in the north of the continent near the\n",
       "equator do not observe DST, while Paraguay and southern parts of Brazil do. The practice of observing daylight     \n",
       "saving time in Oceania is also mixed, with New Zealand and parts of southeastern Australia observing DST, while    \n",
       "most other areas do not.                                                                                           \n",
       "\n",
       "OPTIONS:                                                                                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0mTrue                                                                                                            \n",
       "\u001b[1;33m • \u001b[0mFalse                                                                                                           \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "True                                                                                                               \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "True                                                                                                               \n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"bool_q\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2512aafedc484325adbcf233f2cb65e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566877f670d24cb09968756181232a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16820d3eed1c44dfaa6399fcb04b11a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b04d82d92cb4ed98391af1e1d9fd0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Create a Python program to perform sentiment analysis on product reviews. The program should use natural language  \n",
       "processing techniques like tokenization and n-grams to generate features for sentiment analysis and use a          \n",
       "classification model to assign sentiments to the reviews.                                                          \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       " </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> numpy </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> np</span><span style=\"background-color: #272822\">                                                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> pandas </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> pd</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> nltk</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> nltk.tokenize </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> word_tokenize</span><span style=\"background-color: #272822\">                                                                           </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> matplotlib.pyplot </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> plt</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> nltk.corpus </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> stopwords</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> nltk.stem </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> WordNetLemmatizer </span><span style=\"background-color: #272822\">                                                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> sklearn.feature_extraction.text </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> CountVectorizer</span><span style=\"background-color: #272822\">                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> sklearn.model_selection </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> train_test_split</span><span style=\"background-color: #272822\">                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> sklearn.naive_bayes </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> MultinomialNB</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> sklearn.metrics </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> classification_report</span><span style=\"background-color: #272822\">                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Read in the data</span><span style=\"background-color: #272822\">                                                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">data </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> pd</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">read_csv(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'reviews.csv'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Tokenise the texts</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'tokenised'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'review'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">apply(word_tokenize)</span><span style=\"background-color: #272822\">                                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Remove stopwords</span><span style=\"background-color: #272822\">                                                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">stop_words </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> set(stopwords</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">words(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'english'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'filtered'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'tokenised'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">apply(</span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">lambda</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> x: [val </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> val </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> x </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> val </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">not</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> stop_words])</span><span style=\"background-color: #272822\">                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Normalize words</span><span style=\"background-color: #272822\">                                                                                                 </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">wordnet_lemmatizer </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> WordNetLemmatizer()</span><span style=\"background-color: #272822\">                                                                          </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'normalised'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'filtered'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">apply(</span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">lambda</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> x: [wordnet_lemmatizer</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">lemmatize(val) </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> val </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> x])</span><span style=\"background-color: #272822\">           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Generate features using CountVectorizer</span><span style=\"background-color: #272822\">                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cv </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> CountVectorizer()</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'features'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> cv</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">fit_transform(data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'normalised'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">astype(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'str'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">toarray()</span><span style=\"background-color: #272822\">                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Split into training and test sets</span><span style=\"background-color: #272822\">                                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">X </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'features'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">y </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> data[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'sentiment'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">X_train, X_test, y_train, y_test </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> train_test_split(X, y, test_size</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0.2</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, random_state</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">42</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                         </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Train model</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">model </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> MultinomialNB(alpha</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0.1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">fit(X_train, y_train)</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Predict using the test set</span><span style=\"background-color: #272822\">                                                                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">predictions </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">predict(X_test)</span><span style=\"background-color: #272822\">                                                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Generate a classification report</span><span style=\"background-color: #272822\">                                                                                </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">report </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> classification_report(y_test, predictions)</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(report)</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       " </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">def</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">sentiment_analysis</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(product):</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">     </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">return</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'reviews(7.5 ) have a very positive sentiment'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                        </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   \n",
       "</span></pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Create a Python program to perform sentiment analysis on product reviews. The program should use natural language  \n",
       "processing techniques like tokenization and n-grams to generate features for sentiment analysis and use a          \n",
       "classification model to assign sentiments to the reviews.                                                          \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnumpy\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnp\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpandas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpd\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnltk\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnltk\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtokenize\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword_tokenize\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmatplotlib\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpyplot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mplt\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnltk\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcorpus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstopwords\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnltk\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstem\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mWordNetLemmatizer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msklearn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfeature_extraction\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtext\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCountVectorizer\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msklearn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel_selection\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtrain_test_split\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msklearn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnaive_bayes\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mMultinomialNB\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msklearn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmetrics\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mclassification_report\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Read in the data\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpd\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mread_csv\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mreviews.csv\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Tokenise the texts\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtokenised\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mreview\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mapply\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword_tokenize\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Remove stopwords\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstop_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mset\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstopwords\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwords\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34menglish\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mfiltered\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtokenised\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mapply\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mlambda\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mx\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mval\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mval\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mx\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mval\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mnot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstop_words\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Normalize words\u001b[0m\u001b[48;2;39;40;34m                                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwordnet_lemmatizer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mWordNetLemmatizer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnormalised\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mfiltered\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mapply\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mlambda\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mx\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwordnet_lemmatizer\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlemmatize\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mval\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mval\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mx\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Generate features using CountVectorizer\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcv\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCountVectorizer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mfeatures\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcv\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfit_transform\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnormalised\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mastype\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mstr\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtoarray\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Split into training and test sets\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mX\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mfeatures\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34my\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdata\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34msentiment\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mX_train\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mX_test\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34my_train\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34my_test\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtrain_test_split\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mX\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34my\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtest_size\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0.2\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrandom_state\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m42\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Train model\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mMultinomialNB\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34malpha\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0.1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mX_train\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34my_train\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Predict using the test set\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpredictions\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpredict\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mX_test\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Generate a classification report\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mclassification_report\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34my_test\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpredictions\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mdef\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34msentiment_analysis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mproduct\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m     \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mreturn\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mreviews(7.5 ) have a very positive sentiment\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \n",
       "\u001b[0m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"python_code\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], max_tokens=1000, return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eng-Spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Translate \"Así crecimos nosotros.\" from Spanish to English.                                                        \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "That's how we grew up.                                                                                             \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "Don't mess with him.                                                                                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Translate \"Así crecimos nosotros.\" from Spanish to English.                                                        \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "That's how we grew up.                                                                                             \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "Don't mess with him.                                                                                               \n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"eng_spa\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38a74f5866647028af56f0084c113a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2315695b4ef4c2992de0a57a42f632b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/49401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1d72ad2112456d936e9925716f0cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Concord is of imperfective type in the ergative aspect but perfective in the nominative aspect . Concord is in the \n",
       "imperfective aspect of the nominative type , but supplementary in the perfective aspect . Are these two sentences  \n",
       "paraphrases of each other? OPTIONS:                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>No                                                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Yes                                                                                                             \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "No                                                                                                                 \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "Yes                                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Concord is of imperfective type in the ergative aspect but perfective in the nominative aspect . Concord is in the \n",
       "imperfective aspect of the nominative type , but supplementary in the perfective aspect . Are these two sentences  \n",
       "paraphrases of each other? OPTIONS:                                                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0mNo                                                                                                              \n",
       "\u001b[1;33m • \u001b[0mYes                                                                                                             \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "No                                                                                                                 \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "Yes                                                                                                                \n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"paws\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cd46983fb64c9d97318e3c48ca095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/56402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses... : 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INPUT PROMPT</span>                                                    \n",
       "\n",
       "Who is the most powerful Marvel character in MCU?                                                                  \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">GROUND_TRUTH</span>                                                    \n",
       "\n",
       "Note:I will answer about MCU 1=TLT 2=The Abstracts 3=Alioth 4=Dormammu 5=Arishem                                   \n",
       "\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">MODEL OUTPUT</span>                                                    \n",
       "\n",
       "I am not confident in the truth of their existence. Why not?                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                   \u001b[1;4mINPUT PROMPT\u001b[0m                                                    \n",
       "\n",
       "Who is the most powerful Marvel character in MCU?                                                                  \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mGROUND_TRUTH\u001b[0m                                                    \n",
       "\n",
       "Note:I will answer about MCU 1=TLT 2=The Abstracts 3=Alioth 4=Dormammu 5=Arishem                                   \n",
       "\n",
       "\n",
       "                                                   \u001b[1;4mMODEL OUTPUT\u001b[0m                                                    \n",
       "\n",
       "I am not confident in the truth of their existence. Why not?                                                       \n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"quora\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_instruct_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_instruct_dataset\u001b[49m(num_samples, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpaca\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m example \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]], return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m CAUSAL_LM)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_instruct_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = create_instruct_dataset(num_samples, [\"alpaca\"])\n",
    "example = dataset[0]\n",
    "prediction = eval.generate([example[\"prompt\"]], return_full_text=not CAUSAL_LM)[0]\n",
    "Markdown(template.format(prompt=example[\"prompt\"], \n",
    "                         completion=example[\"completion\"], \n",
    "                         prediction=prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
